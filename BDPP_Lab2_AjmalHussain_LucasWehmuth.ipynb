{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDPP Course lab 2\n",
    "### Machine Learning With Spark ML\n",
    "In this lab assignment, you will complete a project by going through the following steps:\n",
    "1. Get the data.\n",
    "2. Discover the data to gain insights.\n",
    "3. Prepare the data for Machine Learning algorithms.\n",
    "4. Select a model and train it.\n",
    "5. Fine-tune your model.\n",
    "6. Present your solution.\n",
    "\n",
    "Before we start, I suggest you take a look at the following programming guides:\n",
    "- [pyspark SQL and DataFrames](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)\n",
    "- [Machine Learning Library (MLlib) Guide](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "    - [Extracting, transforming and selecting features](https://spark.apache.org/docs/latest/ml-features)\n",
    "    - [Classification and regression](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "    - [Model selection and tuning](https://spark.apache.org/docs/latest/ml-tuning.html)\n",
    "\n",
    "As a dataset, we use the California Housing Prices dataset from the StatLib repository. This dataset was based on data from the 1990 California census. The dataset has the following columns\n",
    "1. `longitude`: a measure of how far west a house is (a higher value is farther west)\n",
    "2. `latitude`: a measure of how far north a house is (a higher value is farther north)\n",
    "3. `housing_,median_age`: the median age of a house within a block (a lower number is a newer building)\n",
    "4. `total_rooms`: total number of rooms within a block\n",
    "5. `total_bedrooms`: total number of bedrooms within a block\n",
    "6. `population`: total number of people residing within a block\n",
    "7. `households`: total number of households, a group of people residing within a home unit, for a block\n",
    "8. `median_income`: median income for households within a block of houses\n",
    "9. `median_house_value`: median house value for households within a block\n",
    "10. `ocean_proximity`: location of the house w.r.t ocean/sea\n",
    "\n",
    "---\n",
    "# 1. Get the data\n",
    "Let's start the lab by loading the dataset. You can find the dataset at `data/housing.csv`. To infer column types automatically, when you are reading the file, you need to set `inferSchema` to true. Moreover, you need to enable the `header` option to read the columns' names from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"lab2\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o75.load.\n: java.io.IOException: No FileSystem for scheme: gs\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-661cda17cd0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#housing = spark.read.load(\"gs://ajmal1/housing.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtraindata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gs://ajmal1/data_frame.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o75.load.\n: java.io.IOException: No FileSystem for scheme: gs\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "#housing = spark.read.load(\"gs://ajmal1/housing.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "traindata = spark.read.load(\"gs://ajmal1/data_frame.csv\",format=\"csv\", sep=\",\", inferSchema=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Discover the data to gain insights\n",
    "Now it is time to take a look at the data. In this step, we are going to take a look at the data in a few different ways:\n",
    "* See the schema and dimension of the dataset\n",
    "* Look at the data itself\n",
    "* Statistical summary of the attributes\n",
    "* Breakdown of the data by the categorical attribute variable\n",
    "* Find the correlation among different attributes\n",
    "* Make new attributes by combining existing attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Schema and dimension\n",
    "Print the schema of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Look at the data\n",
    "Print the first five records of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(longitude=-122.23, latitude=37.88, housing_median_age=41.0, total_rooms=880.0, total_bedrooms=129.0, population=322.0, households=126.0, median_income=8.3252, median_house_value=452600.0, ocean_proximity='NEAR BAY'),\n",
       " Row(longitude=-122.22, latitude=37.86, housing_median_age=21.0, total_rooms=7099.0, total_bedrooms=1106.0, population=2401.0, households=1138.0, median_income=8.3014, median_house_value=358500.0, ocean_proximity='NEAR BAY'),\n",
       " Row(longitude=-122.24, latitude=37.85, housing_median_age=52.0, total_rooms=1467.0, total_bedrooms=190.0, population=496.0, households=177.0, median_income=7.2574, median_house_value=352100.0, ocean_proximity='NEAR BAY'),\n",
       " Row(longitude=-122.25, latitude=37.85, housing_median_age=52.0, total_rooms=1274.0, total_bedrooms=235.0, population=558.0, households=219.0, median_income=5.6431, median_house_value=341300.0, ocean_proximity='NEAR BAY'),\n",
       " Row(longitude=-122.25, latitude=37.85, housing_median_age=52.0, total_rooms=1627.0, total_bedrooms=280.0, population=565.0, households=259.0, median_income=3.8462, median_house_value=342200.0, ocean_proximity='NEAR BAY')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of records with a population of more than 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.filter(housing['population']> 10000).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -121.92|   37.53|               7.0|    28258.0|        3864.0|   12203.0|    3701.0|       8.4045|          451100.0|      <1H OCEAN|\n",
      "|  -117.78|   34.03|               8.0|    32054.0|        5290.0|   15507.0|    5050.0|       6.0191|          253900.0|      <1H OCEAN|\n",
      "|  -117.87|   34.04|               7.0|    27700.0|        4179.0|   15037.0|    4072.0|       6.6288|          339700.0|      <1H OCEAN|\n",
      "|  -117.88|   33.96|              16.0|    19059.0|        3079.0|   10988.0|    3061.0|       5.5469|          265200.0|      <1H OCEAN|\n",
      "|  -118.78|   34.16|               9.0|    30405.0|        4093.0|   12873.0|    3931.0|       8.0137|          399200.0|     NEAR OCEAN|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.filter(housing['population']> 10000).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Statistical summary\n",
    "Print a summary of the table statistics for the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. You can use the `describe` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|housing_median_age|       total_rooms|median_house_value|        population|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|             20640|             20640|             20640|             20640|\n",
      "|   mean|28.639486434108527|2635.7630813953488|206855.81690891474|1425.4767441860465|\n",
      "| stddev| 12.58555761211163|2181.6152515827944|115395.61587441359|  1132.46212176534|\n",
      "|    min|               1.0|               2.0|           14999.0|               3.0|\n",
      "|    max|              52.0|           39320.0|          500001.0|           35682.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.describe(['housing_median_age', 'total_rooms', 'median_house_value', 'population']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the maximum age (`housing_median_age`), the minimum number of rooms (`total_rooms`), and the average of house values (`median_house_value`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|max(housing_median_age)|\n",
      "+-----------------------+\n",
      "|                   52.0|\n",
      "+-----------------------+\n",
      "\n",
      "+----------------+\n",
      "|min(total_rooms)|\n",
      "+----------------+\n",
      "|             2.0|\n",
      "+----------------+\n",
      "\n",
      "+-----------------------+\n",
      "|avg(median_house_value)|\n",
      "+-----------------------+\n",
      "|     206855.81690891474|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "housing.groupBy().max('housing_median_age').show()\n",
    "housing.groupBy().min('total_rooms').show()\n",
    "housing.groupBy().avg('median_house_value').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Breakdown the data by categorical data\n",
    "Print the number of houses in different areas (`ocean_proximity`), and sort them in descending order. You can use [groupBy](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframe#pyspark.sql.DataFrame.groupBy) and [orderBy](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframe#pyspark.sql.DataFrame.orderBy) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|      <1H OCEAN| 9136|\n",
      "|         INLAND| 6551|\n",
      "|     NEAR OCEAN| 2658|\n",
      "|       NEAR BAY| 2290|\n",
      "|         ISLAND|    5|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.groupBy(\"ocean_proximity\").count().orderBy([\"count\"], ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+---------------+-----+\n",
    "|ocean_proximity|count|\n",
    "+---------------+-----+\n",
    "|      <1H OCEAN| 9136|\n",
    "|         INLAND| 6551|\n",
    "|     NEAR OCEAN| 2658|\n",
    "|       NEAR BAY| 2290|\n",
    "|         ISLAND|    5|\n",
    "+---------------+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the average value of the houses (`median_house_value`) in different areas (`ocean_proximity`), and call the new column `avg_value` when printing it. You can use [agg](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframe#pyspark.sql.DataFrame.agg) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|ocean_proximity|         avg_value|\n",
      "+---------------+------------------+\n",
      "|         ISLAND|          380440.0|\n",
      "|       NEAR BAY|259212.31179039303|\n",
      "|     NEAR OCEAN|249433.97742663656|\n",
      "|      <1H OCEAN|240084.28546409807|\n",
      "|         INLAND|124805.39200122119|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# housing.groupBy(\"ocean_proximity\").avg(\"median_house_value\").show()\n",
    "\n",
    "housing \\\n",
    "    .groupBy(\"ocean_proximity\") \\\n",
    "    .agg(avg(\"median_house_value\").alias(\"avg_value\")) \\\n",
    "    .orderBy([\"avg_value\"], ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+---------------+------------------+\n",
    "|ocean_proximity|         avg_value|\n",
    "+---------------+------------------+\n",
    "|         ISLAND|          380440.0|\n",
    "|       NEAR BAY|259212.31179039303|\n",
    "|     NEAR OCEAN|249433.97742663656|\n",
    "|      <1H OCEAN|240084.28546409807|\n",
    "|         INLAND|124805.39200122119|\n",
    "+---------------+------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the above question in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|ocean_proximity|         avg_value|\n",
      "+---------------+------------------+\n",
      "|         ISLAND|          380440.0|\n",
      "|       NEAR BAY|259212.31179039303|\n",
      "|     NEAR OCEAN|249433.97742663656|\n",
      "|      <1H OCEAN|240084.28546409807|\n",
      "|         INLAND|124805.39200122119|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.createOrReplaceTempView(\"df\")\n",
    "\n",
    "spark.sql(\\\n",
    "          \"SELECT ocean_proximity, \\\n",
    "          AVG(median_house_value) as avg_value \\\n",
    "          FROM df \\\n",
    "          GROUP BY ocean_proximity \\\n",
    "          ORDER BY avg_value DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the same output as the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Correlation among attributes\n",
    "Print the correlation among the attributes `housing_median_age`, `total_rooms`, `median_house_value`, and `population`. To do so, first, you need to put these attributes into one vector. Then, compute the standard correlation coefficient (Pearson) between every pair of attributes in this new vector. To make a vector of these attributes, you can use the [VectorAssembler](https://spark.apache.org/docs/latest/ml-features#vectorassembler) Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[41.0,880.0,45260...|\n",
      "|[21.0,7099.0,3585...|\n",
      "|[52.0,1467.0,3521...|\n",
      "|[52.0,1274.0,3413...|\n",
      "|[52.0,1627.0,3422...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"housing_median_age\", \"total_rooms\", \"median_house_value\", \"population\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "housingAttrs = assembler.transform(housing)\n",
    "\n",
    "housingAttrs.select(\"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+--------------------+\n",
    "|            features|\n",
    "+--------------------+\n",
    "|[41.0,880.0,45260...|\n",
    "|[21.0,7099.0,3585...|\n",
    "|[52.0,1467.0,3521...|\n",
    "|[52.0,1274.0,3413...|\n",
    "|[52.0,1627.0,3422...|\n",
    "+--------------------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation matrix:\n",
      "DenseMatrix([[ 1.        , -0.3612622 ,  0.10562341, -0.29624424],\n",
      "             [-0.3612622 ,  1.        ,  0.13415311,  0.85712597],\n",
      "             [ 0.10562341,  0.13415311,  1.        , -0.02464968],\n",
      "             [-0.29624424,  0.85712597, -0.02464968,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "r1 = Correlation.corr(dataset=housingAttrs, column=\"features\", method=\"pearson\").collect()[0]\n",
    "\n",
    "print(\"correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "correlation matrix:\n",
    "DenseMatrix([[ 1.        , -0.3612622 ,  0.10562341, -0.29624424],\n",
    "             [-0.3612622 ,  1.        ,  0.13415311,  0.85712597],\n",
    "             [ 0.10562341,  0.13415311,  1.        , -0.02464968],\n",
    "             [-0.29624424,  0.85712597, -0.02464968,  1.        ]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Combine and make new attributes\n",
    "Now, let's try out various attribute combinations. In the given dataset, the total number of rooms in a block is not very useful if we don't know how many households there are. What we really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful, and we want to compare it to the number of rooms. And the population per household seems like also an interesting attribute combination to look at. To do so, add the three new columns to the dataset as below. We will call the new dataset the `housingExtra`.\n",
    "```\n",
    "rooms_per_household = total_rooms / households\n",
    "bedrooms_per_room = total_bedrooms / total_rooms\n",
    "population_per_household = population / households\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------------+\n",
      "|rooms_per_household|  bedrooms_per_room|population_per_household|\n",
      "+-------------------+-------------------+------------------------+\n",
      "|  6.984126984126984|0.14659090909090908|      2.5555555555555554|\n",
      "|  6.238137082601054|0.15579659106916466|       2.109841827768014|\n",
      "|  8.288135593220339|0.12951601908657123|      2.8022598870056497|\n",
      "| 5.8173515981735155|0.18445839874411302|       2.547945205479452|\n",
      "|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|\n",
      "+-------------------+-------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housingCol1 = housing.withColumn(\"rooms_per_household\", col(\"total_rooms\") / col(\"households\"))\n",
    "housingCol2 = housingCol1.withColumn(\"bedrooms_per_room\", col(\"total_bedrooms\") / col(\"total_rooms\"))\n",
    "housingExtra = housingCol2.withColumn(\"population_per_household\", col(\"population\") / col(\"households\"))\n",
    "\n",
    "housingExtra.select(\"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+-------------------+-------------------+------------------------+\n",
    "|rooms_per_household|  bedrooms_per_room|population_per_household|\n",
    "+-------------------+-------------------+------------------------+\n",
    "|  6.984126984126984|0.14659090909090908|      2.5555555555555554|\n",
    "|  6.238137082601054|0.15579659106916466|       2.109841827768014|\n",
    "|  8.288135593220339|0.12951601908657123|      2.8022598870056497|\n",
    "| 5.8173515981735155|0.18445839874411302|       2.547945205479452|\n",
    "|  6.281853281853282| 0.1720958819913952|      2.1814671814671813|\n",
    "+-------------------+-------------------+------------------------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Prepare the data for Machine Learning algorithms\n",
    "Before going through the Machine Learning steps, let's first rename the label column from `median_house_value` to `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income',\n",
       " 'label',\n",
       " 'ocean_proximity',\n",
       " 'rooms_per_household',\n",
       " 'bedrooms_per_room',\n",
       " 'population_per_household']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renamedHousing = housingExtra.withColumnRenamed(\"median_house_value\", \"label\")\n",
    "renamedHousing.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to separate the numerical attributes from the categorical attribute (`ocean_proximity`) and keep their column names in two different lists. Moreover, since we don't want to apply the same transformations to the predictors (features) and the label, we should remove the label attribute from the list of predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label columns\n",
    "colLabel = \"label\"\n",
    "\n",
    "# categorical columns\n",
    "colCat = \"ocean_proximity\"\n",
    "\n",
    "#numerical columns\n",
    "colNum = list(filter(lambda x: x != colLabel and x !=colCat  , renamedHousing.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income',\n",
       " 'rooms_per_household',\n",
       " 'bedrooms_per_room',\n",
       " 'population_per_household']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Prepare continuse attributes\n",
    "### Data cleaning\n",
    "Most Machine Learning algorithms cannot work with missing features, so we should take care of them. As a first step, let's find the columns with missing values in the numerical attributes. To do so, we can print the number of missing values of each continuous attributes, listed in `colNum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|rooms_per_household|bedrooms_per_room|population_per_household|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n",
      "|        0|       0|                 0|          0|           207|         0|         0|            0|                  0|              207|                       0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, count, col\n",
    "\n",
    "renamedHousing.agg(*[count(when(isnull(c), c)).alias(c) for c in colNum]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n",
    "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|rooms_per_household|bedrooms_per_room|population_per_household|\n",
    "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n",
    "|        0|       0|                 0|          0|           207|         0|         0|            0|                  0|              207|                       0|\n",
    "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observed above, the `total_bedrooms` and `bedrooms_per_room` attributes have some missing values. One way to take care of missing values is to use the `Imputer` Transformer, which completes missing values in a dataset, either using the mean or the median of the columns in which the missing values are located. To use it, you need to create an [Imputer](https://spark.apache.org/docs/latest/ml-features#imputer) instance, specifying that you want to replace each attribute's missing values with the \"median\" of that attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|total_bedrooms|  bedrooms_per_room|\n",
      "+--------------+-------------------+\n",
      "|         129.0|0.14659090909090908|\n",
      "|        1106.0|0.15579659106916466|\n",
      "|         190.0|0.12951601908657123|\n",
      "|         235.0|0.18445839874411302|\n",
      "|         280.0| 0.1720958819913952|\n",
      "+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=[\"total_bedrooms\", \"bedrooms_per_room\"], outputCols=[\"total_bedrooms\", \"bedrooms_per_room\"])\n",
    "imputer.setStrategy(\"median\")\n",
    "\n",
    "model = imputer.fit(renamedHousing)\n",
    "\n",
    "imputedHousing = model.transform(renamedHousing)\n",
    "imputedHousing.select(\"total_bedrooms\", \"bedrooms_per_room\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+--------------+-------------------+\n",
    "|total_bedrooms|  bedrooms_per_room|\n",
    "+--------------+-------------------+\n",
    "|         129.0|0.14659090909090908|\n",
    "|        1106.0|0.15579659106916466|\n",
    "|         190.0|0.12951601908657123|\n",
    "|         235.0|0.18445839874411302|\n",
    "|         280.0| 0.1720958819913952|\n",
    "+--------------+-------------------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|rooms_per_household|bedrooms_per_room|population_per_household|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n",
      "|        0|       0|                 0|          0|             0|         0|         0|            0|                  0|                0|                       0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+-------------------+-----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######## Just checking if the new dataset does not have any missing values ##########\n",
    "imputedHousing.agg(*[count(when(isnull(c), c)).alias(c) for c in colNum]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don't perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the label attributes is generally not required.\n",
    "\n",
    "One way to get all attributes to have the same scale is to use standardization. In standardization, for each value, first, it subtracts the mean value (so standardized values always have a zero mean). Then it divides by the variance so that the resulting distribution has a unit variance. To do this, we can use the `StandardScaler` Estimator. To use `StandardScaler`, again, we need to convert all the numerical attributes into a big vector of features using `VectorAssembler`, and then call `StandardScaler` on that vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+\n",
      "|[-122.23,37.88,41...|[-1.3278030546902...|\n",
      "|[-122.22,37.86,21...|[-1.3228118684350...|\n",
      "|[-122.24,37.85,52...|[-1.3327942409452...|\n",
      "|[-122.25,37.85,52...|[-1.3377854272003...|\n",
      "|[-122.25,37.85,52...|[-1.3377854272003...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=colNum, outputCol=\"features\")\n",
    "\n",
    "featuredHousing = assembler.transform(imputedHousing)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withMean=True)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(featuredHousing)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledHousing = scalerModel.transform(featuredHousing)\n",
    "scaledHousing.select([\"features\", \"scaledFeatures\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+--------------------+--------------------+\n",
    "|            features|      scaledFeatures|\n",
    "+--------------------+--------------------+\n",
    "|[-122.23,37.88,41...|[-1.3278030546902...|\n",
    "|[-122.22,37.86,21...|[-1.3228118684350...|\n",
    "|[-122.24,37.85,52...|[-1.3327942409452...|\n",
    "|[-122.25,37.85,52...|[-1.3377854272003...|\n",
    "|[-122.25,37.85,52...|[-1.3377854272003...|\n",
    "+--------------------+--------------------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x115733d90>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Lucas/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'StandardScaler' object has no attribute '_java_obj'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std: 0.9999757749003724 mean: 6.392693906922685e-15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "median_income = [row['scaledFeatures'] for row in scaledHousing.collect()]\n",
    "print(\"std: \" + str(np.std(median_income)) + \" mean: \" + str(np.mean(median_income)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "std: 0.9999757749003724 mean: 6.392693906922685e-15\n",
    "```\n",
    "__Note__ that std ≈ 1, and mean ≈ 0, so the transformed data has a standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Prepare categorical attributes\n",
    "After imputing and scaling the continuous attributes, we should take care of the categorical attributes. Let's first print the number of distinct values of the categorical attribute `ocean_proximity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renamedHousing.select([\"ocean_proximity\"]).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should het `5` as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String indexer\n",
    "Most Machine Learning algorithms prefer to work with numbers. So let's convert the categorical attribute `ocean_proximity` to numbers. To do so, we can use the [StringIndexer](https://spark.apache.org/docs/latest/ml-features#stringindexer) that encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+\n",
      "|ocean_proximity|ocean_proximity_num|\n",
      "+---------------+-------------------+\n",
      "|       NEAR BAY|                3.0|\n",
      "|       NEAR BAY|                3.0|\n",
      "|       NEAR BAY|                3.0|\n",
      "|       NEAR BAY|                3.0|\n",
      "|       NEAR BAY|                3.0|\n",
      "+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ocean_proximity_num\")\n",
    "idxHousing = indexer.fit(renamedHousing).transform(renamedHousing)\n",
    "idxHousing.select([\"ocean_proximity\", \"ocean_proximity_num\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+---------------+-------------------+\n",
    "|ocean_proximity|ocean_proximity_num|\n",
    "+---------------+-------------------+\n",
    "|       NEAR BAY|                3.0|\n",
    "|       NEAR BAY|                3.0|\n",
    "|       NEAR BAY|                3.0|\n",
    "|       NEAR BAY|                3.0|\n",
    "|       NEAR BAY|                3.0|\n",
    "+---------------+-------------------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this numerical data in any Machine Learning algorithm. You can look at the mapping that this encoder has learned using the `labels` method: \"<1H OCEAN\" is mapped to 0, \"INLAND\" is mapped to 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer.fit(renamedHousing).labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your output should look like this:\n",
    "```\n",
    "['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "Now, convert the label indices built in the last step into one-hot vectors. To do this, you can take advantage of the [OneHotEncoderEstimator](https://spark.apache.org/docs/latest/ml-features#onehotencoderestimator) Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ocean_proximity_num_vec=SparseVector(4, {3: 1.0})),\n",
       " Row(ocean_proximity_num_vec=SparseVector(4, {3: 1.0})),\n",
       " Row(ocean_proximity_num_vec=SparseVector(4, {3: 1.0})),\n",
       " Row(ocean_proximity_num_vec=SparseVector(4, {3: 1.0})),\n",
       " Row(ocean_proximity_num_vec=SparseVector(4, {3: 1.0}))]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"ocean_proximity_num\"], outputCols=[\"ocean_proximity_num_vec\"])\n",
    "\n",
    "model = encoder.fit(idxHousing)\n",
    "ohHousing = model.transform(idxHousing)\n",
    "ohHousing.select(\"ocean_proximity_num_vec\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Pipeline\n",
    "As you can see, many data transformation steps need to be executed in the right order. For example, you called the `Imputer`, `VectorAssembler`, and `StandardScaler` from left to right. However, we can use the `Pipeline` class to define a sequence of Transformers/Estimators, and run them in order. A `Pipeline` is an `Estimator`; thus, after a Pipeline's `fit()` method runs, it produces a `PipelineModel`, which is a `Transformer`.\n",
    "\n",
    "Now, let's create a pipeline called `numPipeline` to call the numerical transformers you built above (`imputer`, `assembler`, and `scaler`) in the right order from left to right, as well as a pipeline called `catPipeline` to call the categorical transformers (`indexer` and `encoder`). Then, put these two pipelines `numPipeline` and `catPipeline` into one pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-------------------+-----------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|   label|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|            features|      scaledFeatures|ocean_proximity_num|ocean_proximity_num_vec|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-------------------+-----------------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|452600.0|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|[-122.23,37.88,41...|[-1.3278030546902...|                3.0|          (4,[3],[1.0])|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+--------+---------------+-------------------+-------------------+------------------------+--------------------+--------------------+-------------------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "numPipeline = Pipeline(stages=[imputer, assembler, scaler])\n",
    "catPipeline = Pipeline(stages=[indexer, encoder])\n",
    "\n",
    "pipeline = Pipeline(stages=[numPipeline, catPipeline])\n",
    "\n",
    "newHousing = pipeline.fit(renamedHousing).transform(renamedHousing)\n",
    "newHousing.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+---------------+-------------------+-------------------+------------------------+--------+--------------------+--------------------+-------------------+-----------------------+\n",
    "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|ocean_proximity|rooms_per_household|  bedrooms_per_room|population_per_household|   label|            features|      scaledFeatures|ocean_proximity_num|ocean_proximity_num_vec|\n",
    "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+---------------+-------------------+-------------------+------------------------+--------+--------------------+--------------------+-------------------+-----------------------+\n",
    "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|       NEAR BAY|  6.984126984126984|0.14659090909090908|      2.5555555555555554|452600.0|[-122.23,37.88,41...|[-1.3278030546902...|                3.0|          (4,[3],[1.0])|\n",
    "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+---------------+-------------------+-------------------+------------------------+--------+--------------------+--------------------+-------------------+-----------------------+\n",
    "only showing top 1 row\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `VectorAssembler` to put all attributes of the final dataset `newHousing` into a big vector (called `final_features`), and finally rename the new column `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newHousing.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income',\n",
       " 'label',\n",
       " 'ocean_proximity',\n",
       " 'rooms_per_household',\n",
       " 'bedrooms_per_room',\n",
       " 'population_per_household',\n",
       " 'features',\n",
       " 'scaledFeatures',\n",
       " 'ocean_proximity_num',\n",
       " 'ocean_proximity_num_vec']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newHousing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|   label|\n",
      "+--------------------+--------+\n",
      "|[-122.23,37.88,41...|452600.0|\n",
      "|[-122.22,37.86,21...|358500.0|\n",
      "|[-122.24,37.85,52...|352100.0|\n",
      "|[-122.25,37.85,52...|341300.0|\n",
      "|[-122.25,37.85,52...|342200.0|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### SHOULD BE THE FIRST ONE #####\n",
    "\n",
    "# va2 = VectorAssembler(inputCols=[column for column in newHousing.columns if column != \"ocean_proximity\"], outputCol=\"final_features\")\n",
    "va2 = VectorAssembler(inputCols=[column for column in newHousing.columns if column != \"ocean_proximity\" and column != \"ocean_proximity_num\"], outputCol=\"final_features\")\n",
    "# va2 = VectorAssembler(inputCols=[\"scaledFeatures\"], outputCol=\"final_features\")\n",
    "\n",
    "temp1 = va2.transform(newHousing)\n",
    "\n",
    "dataset = temp1.withColumn('features', temp1.final_features).select(\"features\",\"label\")\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+--------------------+--------+\n",
    "|            features|   label|\n",
    "+--------------------+--------+\n",
    "|[-1.3278030546902...|452600.0|\n",
    "|[-1.3228118684350...|358500.0|\n",
    "|[-1.3327942409452...|352100.0|\n",
    "|[-1.3377854272003...|341300.0|\n",
    "|[-1.3377854272003...|342200.0|\n",
    "+--------------------+--------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Make a model\n",
    "Here we going to make four different regression models:\n",
    "* Linear regression model\n",
    "* Decision tree regression\n",
    "* Random forest regression\n",
    "* Gradient-booster forest regression\n",
    "\n",
    "__Note:__ Based on how you set the model parameters, you may end up getting different results. The sample outputs provided in this section are based on default parameters.\n",
    "\n",
    "But, before giving the data to train a Machine Learning model, let's first split the data into a training dataset (`trainSet`) with 80% of the whole data, and test dataset (`testSet`) with 20% of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# trainSet, testSet = dataset.<FILL IN>\n",
    "trainSet, testSet = dataset.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16567"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4073"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Linear regression model\n",
    "Now, train a Linear Regression model using the `LinearRegression` class. Then, print the coefficients and intercept of the model, as well as the summary of the model over the training set by calling the `summary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.1898439580724977,0.25588888676456223,-0.011032340151326994,-0.0001720809519210081,-0.0009287373835141284,0.0004644913034230214,3.513043713091677e-05,-0.17222597725898617,1.000015235810978,0.05460859797447862,-2.2946659176292368,-0.009964447512404544,0.1898439580724977,0.25588888676456223,-0.011032340151326994,-0.0001720809519210081,-0.0009287373835141284,0.0004644913034230214,3.513043713091677e-05,-0.17222597725898617,0.05460859797447862,-2.2946659176292368,-0.009964447512404544,0.38035863792846397,0.5465669290600951,-0.1388480430772074,-0.37541434377514493,-0.3895048971482989,0.5260188506215655,0.01343140285009548,-0.32719854742666654,0.13511119387469675,-0.13240123971780904,-0.10349122764207322,-176.68945960787937,-176.2154953202055,-175.7912904548835,-173.08242259784765]\n",
      "Intercept: 202.9512084638021\n",
      "RMSE: 3.329555328712852\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# lr = <FILL IN>\n",
    "# lrModel = lr.<FILL IN>\n",
    "# trainingSummary = lrModel.summary\n",
    "\n",
    "lr = LinearRegression()\n",
    "lrModel = lr.fit(trainSet)\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Coefficients: %s\" % str(<FILL IN>))\n",
    "# print(\"Intercept: %s\" % str(<FILL IN>))\n",
    "# print(\"RMSE: %s\" % str(<FILL IN>))\n",
    "\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "print(\"RMSE: %s\" % str(trainingSummary.rootMeanSquaredError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lrModel.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.LinearRegressionModel"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lrModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be similar to this:\n",
    "```\n",
    "Coefficients: [-56639.642291914686,-57319.32110177786,14002.699108121004,3956.096007902445,2052.9245835397637,-44732.96113748317,43938.699408553475,78191.94760029572,8401.054030871619,16148.787588252462,716.5079271705835,-171681.91204667607,-205191.90252085018,-167767.18573795864,-176316.02615341634]\n",
    "Intercept: 389569.42823539214\n",
    "RMSE: 68419.64567910614\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use `RegressionEvaluator` to measure the root-mean-square-error (RMSE) of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "| 85801.52487999355| 85800.0|[-124.3,41.8,19.0...|\n",
      "|103601.43026349347|103600.0|[-124.3,41.84,17....|\n",
      "| 68401.02336265711| 68400.0|[-124.21,41.77,17...|\n",
      "| 90099.47214626997| 90100.0|[-124.19,40.73,21...|\n",
      "|116100.76426560341|116100.0|[-124.17,40.75,13...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 3.3871250048891177\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "##  make predictions on the test data\n",
    "predictions = lrModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "## select (prediction, true label) and compute test error.\n",
    "evaluator = RegressionEvaluator()\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "\n",
    "print( \"Root Mean Squared Error (RMSE) on test data = \"+ str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be similar to this:\n",
    "```\n",
    "+------------------+-------+--------------------+\n",
    "|        prediction|  label|            features|\n",
    "+------------------+-------+--------------------+\n",
    "|202426.15921405185|94600.0|[-2.3859345407710...|\n",
    "|178061.72929810936|79000.0|[-2.3460050507303...|\n",
    "|190967.64201847612|90100.0|[-2.3060755606895...|\n",
    "|169128.68031587286|69000.0|[-2.3060755606895...|\n",
    "|147641.06793444577|67000.0|[-2.3010843744344...|\n",
    "+------------------+-------+--------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "Root Mean Squared Error (RMSE) on test data = 65751.16868231572\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Decision tree regression\n",
    "Repeat what you have done on the Regression Model to build a Decision Tree model. Use the `DecisionTreeRegressor` to make a model and then measure its RMSE on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "| 84107.53295668549| 85800.0|[-124.3,41.8,19.0...|\n",
      "|111931.88679245283|103600.0|[-124.3,41.84,17....|\n",
      "| 68501.27450980392| 68400.0|[-124.21,41.77,17...|\n",
      "| 95695.24733268672| 90100.0|[-124.19,40.73,21...|\n",
      "|111931.88679245283|116100.0|[-124.17,40.75,13...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 6159.283581340476\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "dtModel = dt.fit(trainSet)\n",
    "\n",
    "predictions = dtModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "## select (prediction, true label) and compute test error.\n",
    "evaluator = RegressionEvaluator()\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "print( \"Root Mean Squared Error (RMSE) on test data = \"+ str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be similar to this:\n",
    "```\n",
    "+------------------+-------+--------------------+\n",
    "|        prediction|  label|            features|\n",
    "+------------------+-------+--------------------+\n",
    "|176434.89089848308|94600.0|[-2.3859345407710...|\n",
    "|144694.84987277354|79000.0|[-2.3460050507303...|\n",
    "|176434.89089848308|90100.0|[-2.3060755606895...|\n",
    "| 137376.0736196319|69000.0|[-2.3060755606895...|\n",
    "|144694.84987277354|67000.0|[-2.3010843744344...|\n",
    "+------------------+-------+--------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "Root Mean Squared Error (RMSE) on test data = 64764.20678010809\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Random forest regression\n",
    "Let's try the test error on a Random Forest Model. You can use the `RandomForestRegressor` to make a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "|107238.67022378262| 85800.0|[-124.3,41.8,19.0...|\n",
      "| 124281.1361730535|103600.0|[-124.3,41.84,17....|\n",
      "|116074.46870678128| 68400.0|[-124.21,41.77,17...|\n",
      "|135390.12083428708| 90100.0|[-124.19,40.73,21...|\n",
      "|149488.96440911625|116100.0|[-124.17,40.75,13...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 26345.111570706864\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "rfModel = rf.fit(trainSet)\n",
    "\n",
    "predictions = rfModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "## select (prediction, true label) and compute test error.\n",
    "evaluator = RegressionEvaluator()\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "print( \"Root Mean Squared Error (RMSE) on test data = \"+ str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be similar to this:\n",
    "```\n",
    "+------------------+-------+--------------------+\n",
    "|        prediction|  label|            features|\n",
    "+------------------+-------+--------------------+\n",
    "|196230.80181972618|94600.0|[-2.3859345407710...|\n",
    "|155845.55604907026|79000.0|[-2.3460050507303...|\n",
    "|167298.00808672755|90100.0|[-2.3060755606895...|\n",
    "| 166686.4796195291|69000.0|[-2.3060755606895...|\n",
    "| 142733.3221992184|67000.0|[-2.3010843744344...|\n",
    "+------------------+-------+--------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "Root Mean Squared Error (RMSE) on test data = 63263.050055074156\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Gradient-boosted tree regression\n",
    "Finally, we want to build a Gradient-boosted Tree Regression model and test the RMSE of the test data. Use the `GBTRegressor` to make the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "| 83846.08683118374| 85800.0|[-124.3,41.8,19.0...|\n",
      "|111868.51892276126|103600.0|[-124.3,41.84,17....|\n",
      "| 67812.30873562481| 68400.0|[-124.21,41.77,17...|\n",
      "| 93488.14221226449| 90100.0|[-124.19,40.73,21...|\n",
      "|114013.55186304271|116100.0|[-124.17,40.75,13...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 5578.952326557389\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gb = GBTRegressor()\n",
    "\n",
    "gbModel = gb.fit(trainSet)\n",
    "\n",
    "predictions = gbModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "## select (prediction, true label) and compute test error.\n",
    "evaluator = RegressionEvaluator()\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "print( \"Root Mean Squared Error (RMSE) on test data = \"+ str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be similar to this:\n",
    "```\n",
    "+------------------+-------+--------------------+\n",
    "|        prediction|  label|            features|\n",
    "+------------------+-------+--------------------+\n",
    "|115613.11054178125|94600.0|[-2.3859345407710...|\n",
    "| 81328.79640266801|79000.0|[-2.3460050507303...|\n",
    "|147905.92250882403|90100.0|[-2.3060755606895...|\n",
    "| 85928.30116721884|69000.0|[-2.3060755606895...|\n",
    "| 81399.31430268366|67000.0|[-2.3010843744344...|\n",
    "+------------------+-------+--------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "Root Mean Squared Error (RMSE) on test data = 53622.69614407029\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Hyperparameter tuning\n",
    "An important task in Machine Learning is model selection or using data to find the best model or parameters for a given task. This is also called tuning. Tuning may be done for individual Estimators such as LinearRegression or for entire Pipelines, which include multiple algorithms, featurization, and other steps. Users can tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately. MLlib supports model selection tools, such as [CrossValidator](https://spark.apache.org/docs/latest/ml-tuning.html#cross-validation). These tools require the following items:\n",
    "* Estimator: algorithm or Pipeline to tune (`setEstimator`)\n",
    "* Set of ParamMaps: parameters to choose from, sometimes called a \"parameter grid\" to search over (`setEstimatorParamMaps`)\n",
    "* Evaluator: metric to measure how well a fitted Model does on held-out test data (`setEvaluator`)\n",
    "\n",
    "`CrossValidator` begins by splitting the dataset into a set of folds, which are used as separate training and test datasets. For example, with `k=3` folds, `CrossValidator` will generate 3 (training, test) dataset pairs, each of which uses 2/3 of the data for training and 1/3 for testing. To evaluate a particular `ParamMap`, `CrossValidator` computes the average evaluation metric for the 3 Models produced by fitting the Estimator on the three different (training, test) dataset pairs. After identifying the best `ParamMap`, `CrossValidator` finally re-fits the Estimator using the best ParamMap and the entire dataset.\n",
    "\n",
    "Below, use the `CrossValidator` to select the best Random Forest model. To do so, you need to define a grid of parameters. Let's say we want to search among the different number of trees (1, 5, and 10), and various tree depth (5, 10, and 15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|        prediction|   label|            features|\n",
      "+------------------+--------+--------------------+\n",
      "| 89138.24928639391| 85800.0|[-124.3,41.8,19.0...|\n",
      "|100434.17475728155|103600.0|[-124.3,41.84,17....|\n",
      "| 64777.36943907157| 68400.0|[-124.21,41.77,17...|\n",
      "| 89138.24928639391| 90100.0|[-124.19,40.73,21...|\n",
      "|113250.37037037036|116100.0|[-124.17,40.75,13...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 5996.397739384252\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [1, 5, 10]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator().setMetricName(\"rmse\")\n",
    "\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator,numFolds=3)\n",
    "cvModel = cv.fit(trainSet)\n",
    "\n",
    "predictions = cvModel.transform(testSet)\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "print( \"Root Mean Squared Error (RMSE) on test data = \"+ str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+------------------+-------+--------------------+\n",
    "|        prediction|  label|            features|\n",
    "+------------------+-------+--------------------+\n",
    "|          117870.0|94600.0|[-2.3859345407710...|\n",
    "| 94230.00865800866|79000.0|[-2.3460050507303...|\n",
    "|138148.57142857142|90100.0|[-2.3060755606895...|\n",
    "|120944.28571428572|69000.0|[-2.3060755606895...|\n",
    "| 77710.73015873015|67000.0|[-2.3010843744344...|\n",
    "+------------------+-------+--------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "Root Mean Squared Error (RMSE) on test data = 51427.20882245195\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Custom transformer\n",
    "At the end of part two, we added extra columns to the `housing` dataset. Here, we are going to implement a Transformer to do the same task. The Transformer should take the name of two input columns `inputCol1` and `inputCol2`, as well as the name of output column `outputCol`. It, then, computes `inputCol1` divided by `inputCol2`, and adds its result as a new column to the dataset. You can get help from [here](https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml).\n",
    "\n",
    "First, define the given parameters of the Transformer and implement a method to validate their schemas (`StructType`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "\n",
    "class MyTransformer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    def __init__(self, inputCol1, inputCol2, outputCol):\n",
    "        self.inputCol1 = inputCol1\n",
    "        self.inputCol2 = inputCol2\n",
    "        self.outputCol = outputCol\n",
    "        \n",
    "    def transform(self, dataset):\n",
    "        return dataset.withColumn(self.outputCol, dataset[self.inputCol1] / dataset[self.inputCol2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|rooms_per_household|\n",
      "+-------------------+\n",
      "|  6.984126984126984|\n",
      "|  6.238137082601054|\n",
      "|  8.288135593220339|\n",
      "| 5.8173515981735155|\n",
      "|  6.281853281853282|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mytransformer = MyTransformer(\"total_rooms\",'households','rooms_per_household')\n",
    "mytransformer.transform(housing).select(\"rooms_per_household\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should look like this:\n",
    "```\n",
    "+-------------------+\n",
    "|rooms_per_household|\n",
    "+-------------------+\n",
    "|  6.984126984126984|\n",
    "|  6.238137082601054|\n",
    "|  8.288135593220339|\n",
    "| 5.8173515981735155|\n",
    "|  6.281853281853282|\n",
    "+-------------------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
